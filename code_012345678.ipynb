{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "0kV_tscbLsRq"
      },
      "source": [
        "# Assignment 1 for FIT5212, Semester 1\n",
        "\n",
        "**Student Name:**  Han Li\n",
        "\n",
        "**Student ID:**  32710542"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OBeHKhlsLsRr"
      },
      "source": [
        "## Part 1:  Text Classification\n",
        "\n",
        "General comments and any shared processing here."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Set up\n",
        "##### Below block is for loading all libraries required for the project and preliminary set up including device and seed\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#For data loading and manipulation\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "#For data visulisation\n",
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "#pip install plotly\n",
        "import plotly.express as px\n",
        "import plotly as py\n",
        "\n",
        "#For preprocssing\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer,CountVectorizer\n",
        "import nltk\n",
        "from nltk import PorterStemmer,WordNetLemmatizer,wordpunct_tokenize,word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "import spacy\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer, PorterStemmer\n",
        "from nltk.corpus import stopwords\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
        "import re\n",
        "\n",
        "#For Modelling\n",
        "#pip install xgboost\n",
        "#pip install lightgbm\n",
        "from xgboost import XGBClassifier\n",
        "#import lightgbm as lgb\n",
        "from sklearn.svm import SVC \n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.ensemble import RandomForestClassifier,AdaBoostClassifier,BaggingClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "#For RNN Modelling\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from collections import Counter\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchtext.data.utils import get_tokenizer \n",
        "from collections import Counter\n",
        "from torchtext.vocab import Vocab\n",
        "import re\n",
        "import string\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "#For performance metrics, data spliting\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.metrics import (confusion_matrix,f1_score,ConfusionMatrixDisplay,\n",
        "                             precision_score,accuracy_score,\n",
        "                             recall_score,matthews_corrcoef,precision_recall_curve)\n",
        "\n",
        "#For Model tuning\n",
        "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV,train_test_split\n",
        "\n",
        "\n",
        "#For Topic Modelling\n",
        "import pyLDAvis\n",
        "import pyLDAvis.sklearn\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "from plotly.offline import plot\n",
        "import plotly.graph_objects as go\n",
        "import plotly.express as px\n",
        "\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "\n",
        "#Utilities\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore', category=DeprecationWarning)\n",
        "warnings.filterwarnings('ignore', category=UserWarning)\n",
        "warnings.filterwarnings('ignore', category=FutureWarning)\n",
        "import random\n",
        "import os\n",
        "import re"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Seed for reprodiction\n",
        "SEED = 23\n",
        "def seeding(SEED = SEED):\n",
        "    \"\"\"SEED: int number define by variable SEED\"\"\"\n",
        "    np.random.seed(SEED)\n",
        "    random.seed(SEED)\n",
        "    torch.manual_seed(SEED)\n",
        "seeding() \n",
        "\n",
        "# Setting up cuda for Colab GPU runtime and mps for ARM enviroment\n",
        "# The code will be test under macos arm with mps support and Colab GPU runtime with cuda support\n",
        "DEVICE = torch.device('cuda' if \n",
        "torch.cuda.is_available() else 'cpu')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#Assert train.csv and test.csv are in working directory\n",
        "train_dir = (os.getcwd() +'/train.csv' if 'train.csv' in os.listdir() else print('training set not in working directory'))\n",
        "test_dir = (os.getcwd() +'/test.csv' if 'test.csv' in os.listdir() else print('testing set not in working directory'))\n",
        "\n",
        "#Load datasets\n",
        "df_train = pd.read_csv(train_dir)\n",
        "df_test = pd.read_csv(test_dir)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#Inspect train and test data\n",
        "df_train.sample(5)\n",
        "df_test.sample(5)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Preprocessing\n",
        "## For easy access, I will wrap all necessary steps for preprocessing in 1 single class\n",
        "#### Step 1: Training,Validation split (df_train_val_split)\n",
        "##### This is to split the entire training set into training and validation set basic on task requirement\n",
        "\n",
        "\n",
        "#### Step 2: preprocessing(preprocess) \n",
        "##### This is to preprocess the data using 2 different method\n",
        "##### Method 1: Text to be lower, remove stopword, paranthesis and other special charaters,spaces and apply porter stemmer and tfidf vectorizer\n",
        "##### Method 2: Text to be lower, remove special charaters,spaces and apply lemmatizer and countvectorizer\n",
        "\n",
        "#### Step 3:Clean(clean) \n",
        "##### This is to combine the above method and apply the cleaning to each input(text) of the datasett"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class preprocesser:\n",
        "    \"\"\"\n",
        "    How to use:\n",
        "    Initialize preprocesser \n",
        "    e.g. prep = preporcesser()\n",
        "\n",
        "    Obtain X,y for training and validation set using only the clean function \n",
        "    e.g. X_train,X_val,y_train,y_val = prep.clean(task = 'InformationTheory',size = 'sample',\n",
        "                                                input = 'abstract',method = 2)\n",
        "    \"\"\"\n",
        "\n",
        "    class LemmaTokenizer(object):\n",
        "        def __init__(self):\n",
        "            self.wnl = WordNetLemmatizer()\n",
        "\n",
        "        def __call__(self, doc):\n",
        "            return [self.wnl.lemmatize(t) for t in word_tokenize(doc)]\n",
        "\n",
        "\n",
        "    class SteamTokenizer(object):\n",
        "        def __init__(self):\n",
        "            self.ps = PorterStemmer()\n",
        "\n",
        "        def __call__(self, doc):\n",
        "            return [self.ps.stem(t) for t in word_tokenize(doc)]\n",
        "        \n",
        "    def __init__(self):\n",
        "        self.df = df_train\n",
        "        self.df_test = df_test\n",
        "        self.lemma = self.LemmaTokenizer()\n",
        "        self.steam  = self.SteamTokenizer()\n",
        "\n",
        "        self.stem_vectorizer = TfidfVectorizer(\n",
        "                                  tokenizer=self.steam)\n",
        "        self.LemmaTokenizer = CountVectorizer(\n",
        "            tokenizer=self.lemma\n",
        ")\n",
        "\n",
        "    def df_train_val_split(self, Text, Label, Size):\n",
        "        \n",
        "        \"\"\"\n",
        "        Text:Dependent variables can be choose from ['InformationTheory','ComputationalLinguistics','ComputerVision']\n",
        "\n",
        "        Label:independent variable that can be choose from ['title','abstract']\n",
        "\n",
        "        Size = select from ['full','sample'] to return either 10% or entire training set\n",
        "\n",
        "        df_train_1k, df_val_1k: The first 1000 sample of the df_train set(900 inputs), and split 10% as validation set(100 inputs)\n",
        "\n",
        "        df_train_all, df_val_all: The entire training data(112500 inputs) excluding 10% as validation set(12500 inputs)\n",
        "\n",
        "        Return: X_train,X_val,y_train,y_val the training and validation value as list for both X and y variables\n",
        "        \"\"\"\n",
        "        # Assert inputs meets function requirement\n",
        "        assert Text in [\n",
        "            'title', 'abstract'], 'Please ensure input is choosen from title and abstract'\n",
        "        assert Label in ['InformationTheory', 'ComputationalLinguistics',\n",
        "                        'ComputerVision'], 'Please ensure column name you wish to predict are correct'\n",
        "        assert Size in ['full', 'sample'], 'choose only full or sample dataset'\n",
        "        np.random.RandomState(SEED) \n",
        "        if Size == 'sample':\n",
        "            df_1k = self.df.sample(1000).reset_index(drop=True)\n",
        "            df_train_1k, df_val_1k = train_test_split(df_1k, test_size=.1)\n",
        "            X_train = df_train_1k[Text].tolist()\n",
        "            X_val = df_val_1k[Text].tolist()\n",
        "            y_train = df_train_1k[Label].tolist()\n",
        "            y_val = df_val_1k[Label].tolist()\n",
        "            \n",
        "            \n",
        "\n",
        "        elif Size == 'full':\n",
        "            df_train_all, df_val_all = train_test_split(self.df, test_size=.1)\n",
        "            X_train = df_train_all[Text].tolist()\n",
        "            X_val = df_val_all[Text].tolist()\n",
        "            y_train = df_train_all[Label].tolist()\n",
        "            y_val = df_val_all[Label].tolist()\n",
        "\n",
        "        X_test = self.df_test[Text].tolist()\n",
        "        y_test = self.df_test[Label].tolist()\n",
        "        return X_train, X_val, X_test,y_train,y_val,y_test\n",
        "\n",
        "\n",
        "    def preprocess(self,text,Method = 1):\n",
        "        \"\"\"\n",
        "        text :raw text from dataset input\n",
        "        method: int 1 or 2, refer to differetn preprocessing method\n",
        "        Method 1: Text to be lower, remove stopword, paranthesis and other special charaters,spaces and apply porter stemmer and tfidf vectorizer\n",
        "        Method 2:  Remove special charaters,spaces and apply lemmatizer and countvectorizer\n",
        "        \"\"\"\n",
        "        assert Method in [1,2],'Please select 1 or 2 only'\n",
        "        STOPWORDS = set(stopwords.words('english'))\n",
        "\n",
        "        \n",
        "        # Remove Special characters \\n\n",
        "        text = re.sub(r'\\n','',text)\n",
        "        text = re.sub(' +', ' ', text)  # remove multiple spaces\n",
        "        text = text.strip() \n",
        "\n",
        "        if Method == 1:\n",
        "            # Lower\n",
        "            text = text.lower()\n",
        "            # remove non alphanumeric chars\n",
        "            text = re.sub('[^A-Za-z0-9]+', ' ', text)\n",
        "            # Remove stopwords\n",
        "            pattern = re.compile(r'\\b(' + r'|'.join(STOPWORDS) + r')\\b\\s*')\n",
        "            text = pattern.sub('', text)\n",
        "            # Remove words in paranthesis\n",
        "            text = re.sub(r'\\([^)]*\\)', '', text)\n",
        "\n",
        "        else:\n",
        "            # Removing punctuations,underscrore '_'\n",
        "            text = re.sub(r\"([-;;.,!?<=>])\", r\" \\1 \", text)\n",
        "\n",
        "        return text\n",
        "    \n",
        "    def clean(self,Text,Size,Label,Method):\n",
        "        \"\"\"\n",
        "        Task,input,size = require argument for df_train_val_split\n",
        "        method = require argument ofr preprocess\n",
        "        \"\"\"\n",
        "        X_train, X_val, X_test,y_train,y_val,y_test= self.df_train_val_split(Text,Label,Size)\n",
        "\n",
        "        if Method == 1:\n",
        "            X_train = [self.preprocess(i,Method =1 ) for i in X_train]\n",
        "            X_train = self.stem_vectorizer.fit_transform(X_train).astype(np.float64)\n",
        "            X_val = [self.preprocess(i,Method =1) for i in X_val]\n",
        "            X_val = self.stem_vectorizer.transform(X_val).astype(np.float64)\n",
        "            X_test = [self.preprocess(i,Method =1 ) for i in X_test]\n",
        "            X_test = self.stem_vectorizer.transform(X_test).astype(np.float64)\n",
        "        elif Method == 2:\n",
        "            X_train = [self.preprocess(i,Method =2 ) for i in X_train]\n",
        "            X_train = self.LemmaTokenizer.fit_transform(X_train).astype(np.float64)\n",
        "            X_val = [self.preprocess(i,Method =2 ) for i in X_val]\n",
        "            X_val = self.LemmaTokenizer.transform(X_val).astype(np.float64)\n",
        "            X_test = [self.preprocess(i,Method =2 ) for i in X_test]\n",
        "            X_test = self.LemmaTokenizer.transform(X_test).astype(np.float64)\n",
        "        y_train = np.asarray(y_train)\n",
        "        y_val =np.asanyarray(y_val)\n",
        "        y_test=np.asanyarray(y_test)\n",
        "        return X_train, X_val, X_test,y_train,y_val,y_test"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Training"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "x6_LZOiILsRt"
      },
      "source": [
        "### Part 1A: Statistical Method"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### All functions are wrapped in class training as above\n",
        "#### Step 1: Model Selection(model_selection)\n",
        "##### Using accuracy as metric, I will run a k fold cross validation for each model in the model dictionary, and return the best model base on the average accuracy over the k runs\n",
        "\n",
        "#### Step 2: Fine Tuning (fine_tuning) \n",
        "##### Using the best model from step 1, this function will conduct a grid serach to fine the best parameter of the best model. Returing a predition of y, i.e. y_pred\n",
        "##### Improvement: To reduce runtime, we will only choose the most accuracy model from sample set and run it in testing set and full set\n",
        "\n",
        "#### Step 3:Drawing the Confusion Matrix and Precision recall curve (draw_metrix) \n",
        "##### This is draw the confusion matrix and PR Curve between y_pred and y_true. Also display the f1,precison,recall and accuracy score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class training:\n",
        "    \"\"\"\n",
        "    How to use:\n",
        "    Initialize training \n",
        "    e.g. train = training()\n",
        "\n",
        "    Using draw_metric it will automatecally draw the Confusion matrix and precision recall curve\n",
        "    your can also assign y_pred to it to obtain the prediction y\n",
        "    e.g. y_pred = tra.draw_metric(CV = 2,X = X_train,Y = y_train, X_val = X_val,y_val = y_val,\n",
        "                                i = i,m = m,t = t,s =s)\n",
        "    \n",
        "    \"\"\"\n",
        "    def __init__(self):\n",
        "        \"\"\"\n",
        "        In here we also defined 2 dictionals to locate the require model and parameter as needed\n",
        "        \"\"\"\n",
        "        self.models = {\n",
        "    'XGBClassifier': XGBClassifier(),\n",
        "    'SVM':SVC(),\n",
        "    'BAGGING':BaggingClassifier()}\n",
        "        \n",
        "        self.param_grid = {\n",
        "        'XGBClassifier': {\n",
        "            'learning_rate': np.arange(0.001, 1, 0.05),\n",
        "            'subsample': np.arange(0, 1, 0.3),\n",
        "            'colsample_bytree': np.arange(0, 1, 0.3),\n",
        "            'learning_rate':np.arange(0.0001,0.1,0.05)\n",
        "\n",
        "        },\n",
        "        'SVM': {\n",
        "        'C': [0.1, 1, 10],\n",
        "        'kernel': ['linear', 'rbf']\n",
        "        },\n",
        "         'BAGGING': {\n",
        "        'n_estimators': [10, 50, 100],\n",
        "        'max_samples': [0.5, 0.75, 1.0],\n",
        "        'max_features': [0.5, 0.75, 1.0],\n",
        "        'bootstrap': [True, False]\n",
        "    }\n",
        "        \n",
        "        }\n",
        "\n",
        "    def model_selection(self,CV,X_train,y_train):\n",
        "        \"\"\"\n",
        "        CV:number of Cross validation\n",
        "        NEED TO FIX\n",
        "        \"\"\"\n",
        "        cv_df = pd.DataFrame(index = range(CV *len(self.models)))\n",
        "        entries = []\n",
        "        for mod_name,model in self.models.items():\n",
        "            f1 = cross_val_score(model,X_train,y_train,scoring = 'f1',cv = CV)\n",
        "            for cv_idx,f1 in enumerate(f1):\n",
        "                entries.append((mod_name,cv_idx,f1))\n",
        "        cv_df  = pd.DataFrame(entries,columns=['model_name', 'fold_idx', 'f1'])\n",
        "        best_model_name = cv_df.groupby('model_name')['f1'].std().sort_values(ascending=False).index[0]\n",
        "        return best_model_name\n",
        "    \n",
        "    def fine_tuning(self,CV,best_model_name,X_val,y_val):\n",
        "        \"\"\" \n",
        "        CV:number of Cross validation\n",
        "        X_train,y_train,X_val,y_val,X_test,y_test: tarining,validation and testing set respectively\n",
        "        \"\"\"\n",
        "        randomized_scv = RandomizedSearchCV(\n",
        "            estimator=self.models[best_model_name],\n",
        "            param_distributions = self.param_grid[best_model_name],\n",
        "            n_iter = 10,\n",
        "            scoring= 'accuracy',\n",
        "            cv = CV,\n",
        "            random_state= 1,\n",
        "            n_jobs = -1\n",
        "        ).fit(X_val,y_val)\n",
        "        best_model_tuned = randomized_scv.best_estimator_\n",
        "        #best_model.fit(X_val,y_val)\n",
        "        #y_pred = best_model.predict(X_test)\n",
        "        return best_model_tuned\n",
        "    \n",
        "    def draw_metric(self,best_model_tuned,X_val,y_val,X_test,y_test ,t= None,m = None,l = None,s =None):\n",
        "        #best_model_tuned = self.fine_tuning(CV,X_train,y_train,X_val,y_val,X_test,y_test)\n",
        "        best_model_tuned.fit(X_val,y_val)\n",
        "        y_pred = best_model_tuned.predict(X_test)\n",
        "        print(f\"The F1 Score for this setting is {f1_score(y_test,y_pred):.3f}\")\n",
        "        print(f\"The recall for this setting is {recall_score(y_test,y_pred):.3f}\")\n",
        "        print(f\"The Precision for this setting is {precision_score(y_test,y_pred):.3f}\")\n",
        "        print(f\"The accuracy for this setting is {accuracy_score(y_test,y_pred):.3f}\")\n",
        "        \n",
        "        cm = confusion_matrix(y_test,y_pred)\n",
        "        fig,(ax1,ax2) = plt.subplots(1,2,figsize = (15,5))\n",
        "        if l != None:\n",
        "            fig.suptitle(f\"Text: {t} Method: {m}, Label: {l}, Size: {s}\")\n",
        "        ax1.imshow(cm,cmap = 'Spectral')\n",
        "        ax1.set_title('Confusion Matrix')\n",
        "        ax1.set_xlabel('Predicted Label')\n",
        "        ax1.set_ylabel('True Label')\n",
        "        for i in range(cm.shape[0]):\n",
        "            for j in range(cm.shape[1]):\n",
        "                ax1.text(j, i, str(cm[i, j]), ha='center', va='center')\n",
        "        ax1.grid(False)\n",
        "        precision, recall, thresholds = precision_recall_curve(y_test, y_pred)\n",
        "        ax2.fill_between(recall, precision)\n",
        "        ax2.set_ylabel(\"Precision\")\n",
        "        ax2.set_xlabel(\"Recall\")\n",
        "        ax2.set_title(\"Precision-Recall curve\")\n",
        "        ax2.plot(recall,precision)\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "        return y_pred"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Model Selection\n",
        "Using F1 as the socring metric, I iter the model selection method over all combination for sample size and count which model returned as best model the most. We will then tune only this model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "tr =  training()\n",
        "prep = preprocesser()\n",
        "text = ['title', 'abstract']\n",
        "method = [1,2]\n",
        "label = ['InformationTheory', 'ComputationalLinguistics','ComputerVision']\n",
        "size = ['sample']\n",
        "model_counter = {}\n",
        "for t in text:\n",
        "    for m in method:\n",
        "        for l in label:\n",
        "            for s in size:\n",
        "                X_train, X_val, X_test,y_train,y_val,y_test= prep.clean(Text =t ,Method = m,Label = l,Size =s)\n",
        "                best_model_name = tr.model_selection(CV = 5 ,X_train = X_train,y_train = y_train)\n",
        "                if best_model_name not in model_counter.keys():\n",
        "                    model_counter[best_model_name] = 1\n",
        "                else:\n",
        "                    model_counter[best_model_name] +=1\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model_counter"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Following above,we will be using SVM as our main prediction model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#  lots of Python code here\n",
        "Text = ['title', 'abstract']\n",
        "Method = [1,2]\n",
        "Label = ['InformationTheory', 'ComputationalLinguistics','ComputerVision']\n",
        "Size = ['sample','full']\n",
        "Round = 0\n",
        "results_df = pd.DataFrame()\n",
        "best_model_name = max(model_counter,key = model_counter.get)\n",
        "for t in Text:\n",
        "    for m in Method:\n",
        "        for l in Label:\n",
        "            for s in Size:\n",
        "                Round+=1\n",
        "                X_train, X_val, X_test,y_train,y_val,y_test= prep.clean(Text = t,Method = m,Label = l,Size =s)\n",
        "                print('#' * 80)\n",
        "                print('ITER START')\n",
        "                print(f\"Iter{Round} Text: {t}, Label: {l}, Size: {s}, Method: {m}\")\n",
        "                best_model_tuned = tr.fine_tuning(CV = 5 ,best_model_name=best_model_name,X_val= X_val,y_val = y_val)\n",
        "                y_pred = tr.draw_metric(X_val = X_val,y_val = y_val,X_test=X_test,y_test = y_test,\n",
        "                                t = t,m = m,l = l, s=s,best_model_tuned=best_model_tuned)\n",
        "                print('ITER FINISH')\n",
        "                print('#' * 80)\n",
        "                print('   '* 80)\n",
        "                results_df = results_df.append({\"Iter\":Round,'Text': t , 'Label': l, 'Size': s, 'Method': m, \n",
        "                                                'f1_score': f1_score(y_test,y_pred), \"recall\":recall_score(y_test,y_pred),\n",
        "                                                'Precision':precision_score(y_test,y_pred),\n",
        "                                                'accuracy':accuracy_score(y_test,y_pred),\n",
        "                                                'Algorithm':\"Statistic\"}, ignore_index=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "results_df.sort_values('f1_score',ascending=False)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "0OLEe794LsRu"
      },
      "source": [
        "### Part 1B: RNN Method\n",
        "\n",
        "For RNN method, I will reuse some of the code above mainly preprocesser class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "dataset = pd.read_csv('train.csv')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "X_train, X_val, X_test,y_train,y_val,y_test = prep.df_train_val_split(Label = 'InformationTheory',Text = 'title',Size = 'full')\n",
        "X_train = np.array(X_train)\n",
        "X_val = np.array(X_val)\n",
        "X_test = np.array(X_test)\n",
        "y_train = np.array(y_train)\n",
        "y_val = np.array(y_val)\n",
        "y_test = np.array(y_test)\n",
        "\n",
        "preprocessed_data_train = [prep.preprocess(text,Method = 1) for text in X_train]\n",
        "preprocessed_data_val = [prep.preprocess(text,Method = 1) for text in X_val]\n",
        "preprocessed_data_test = [prep.preprocess(text,Method = 1) for text in X_test]\n",
        "\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(preprocessed_data_train)\n",
        "\n",
        "encoded_docs_train = tokenizer.texts_to_sequences(preprocessed_data_train)\n",
        "max_length_train = max([len(sentence) for sentence in encoded_docs_train])\n",
        "padded_docs_train = pad_sequences(encoded_docs_train, maxlen=max_length_train, padding='post')\n",
        "\n",
        "encoded_docs_val = tokenizer.texts_to_sequences(preprocessed_data_val)\n",
        "max_length_val = max([len(sentence) for sentence in encoded_docs_val])\n",
        "padded_docs_val = pad_sequences(encoded_docs_val, maxlen=max_length_train, padding='post')\n",
        "\n",
        "encoded_docs_test = tokenizer.texts_to_sequences(preprocessed_data_test)\n",
        "max_length_test = max([len(sentence) for sentence in encoded_docs_test])\n",
        "padded_docs_test = pad_sequences(encoded_docs_test, maxlen=max_length_train, padding='post')\n",
        "\n",
        "max_length = max(max_length_train, max_length_val, max_length_test)\n",
        "\n",
        "# Convert to PyTorch tensors\n",
        "X_train = torch.LongTensor(padded_docs_train)\n",
        "y_train = torch.FloatTensor(y_train)\n",
        "# Convert to PyTorch tensors\n",
        "X_val = torch.LongTensor(padded_docs_val)\n",
        "y_val = torch.FloatTensor(y_val)\n",
        "# Convert to PyTorch tensors\n",
        "X_test = torch.LongTensor(padded_docs_test)\n",
        "y_test = torch.FloatTensor(y_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Extract the text column and the label column\n",
        "text_data = dataset['title'].sample(10000,random_state=SEED).values\n",
        "label_data = dataset['ComputerVision'].sample(10000,random_state=SEED).values\n",
        "\n",
        "text_data_test = dataset['title'].values\n",
        "label_data_test = dataset['ComputerVision'].values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Step 2: Preprocess the text data\n",
        "def preprocess(text):\n",
        "    # Remove punctuation and convert to lowercase\n",
        "    text = text.translate(str.maketrans('', '', string.punctuation)).lower()\n",
        "    # Remove numbers\n",
        "    text = re.sub(r'\\d+', '', text)\n",
        "    # Tokenize the text\n",
        "    tokens = word_tokenize(text)\n",
        "    # Remove stop words\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    filtered_tokens = [word for word in tokens if word not in stop_words]\n",
        "    # Lemmatize the text\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    lem_tokens = [lemmatizer.lemmatize(token) for token in filtered_tokens]\n",
        "    # Return the preprocessed text as a string\n",
        "    return ' '.join(lem_tokens)\n",
        "\n",
        "preprocessed_data = [preprocess(text) for text in text_data]\n",
        "preprocessed_data_test = [preprocess(text) for text in text_data_test]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "\n",
        "# Step 3: Convert the preprocessed data into numerical data\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(preprocessed_data)\n",
        "vocab_size = len(tokenizer.word_index) + 1\n",
        "encoded_docs = tokenizer.texts_to_sequences(preprocessed_data)\n",
        "max_length = max([len(sentence) for sentence in encoded_docs])\n",
        "padded_docs = pad_sequences(encoded_docs, maxlen=max_length, padding='post')\n",
        "\n",
        "# Step 3: Convert the preprocessed data into numerical data\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(preprocessed_data_test)\n",
        "vocab_size_test = len(tokenizer.word_index) + 1\n",
        "encoded_docs_test = tokenizer.texts_to_sequences(preprocessed_data_test)\n",
        "max_length_test = max([len(sentence) for sentence in encoded_docs])\n",
        "padded_docs_test = pad_sequences(encoded_docs_test, maxlen=max_length, padding='post')\n",
        "max_length = max_length + max_length_test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Convert to PyTorch tensors\n",
        "X = torch.LongTensor(padded_docs)\n",
        "y = torch.FloatTensor(label_data)\n",
        "\n",
        "# Convert to PyTorch tensors\n",
        "X_test = torch.LongTensor(padded_docs)\n",
        "y_test = torch.FloatTensor(label_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Step 4: Split the data into training and testing sets\n",
        "#X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.1, random_state=42)\n",
        "\n",
        "# Create PyTorch datasets and dataloaders\n",
        "train_data = TensorDataset(X_train, y_train)\n",
        "valid_data = TensorDataset(X_val, y_val)\n",
        "test_data = TensorDataset(X_test, y_test)\n",
        "\n",
        "batch_size = 64\n",
        "train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
        "valid_loader = DataLoader(valid_data,batch_size=batch_size,shuffle = True )\n",
        "test_loader = DataLoader(test_data, batch_size=batch_size, shuffle=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Set hyperparameters\n",
        "embedding_dim = 100\n",
        "hidden_dim = 128\n",
        "num_layers = 2\n",
        "dropout = 0.2\n",
        "\n",
        "# Create an instance of the LSTMModel class\n",
        "model = LSTMModel(vocab_size, embedding_dim, hidden_dim, num_layers, dropout)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Set training hyperparameters\n",
        "learning_rate = 0.001\n",
        "num_epochs = 10\n",
        "\n",
        "# Set loss function and optimizer\n",
        "criterion = nn.BCELoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "# Train the model\n",
        "for epoch in range(num_epochs):\n",
        "    for inputs, labels in train_loader:\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, labels.float())\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "# Evaluate the model\n",
        "with torch.no_grad():\n",
        "    num_correct = 0\n",
        "    num_total = 0\n",
        "    for inputs, labels in test_loader:\n",
        "        outputs = model(inputs)\n",
        "        predictions = (outputs >= 0.5).long()\n",
        "        num_correct += torch.sum(predictions == labels)\n",
        "        num_total += len(labels)\n",
        "    accuracy = float(num_correct) / num_total\n",
        "    print(f1_score(predictions,labels))\n",
        "    print('Test accuracy: {:.2f}%'.format(accuracy * 100))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader\n",
        "from sklearn.metrics import f1_score, precision_score, recall_score\n",
        "from itertools import product\n",
        "\n",
        "# Step 5: Define the LSTM model\n",
        "class LSTMModel(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim, n_layers, dropout):\n",
        "        super().__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
        "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, num_layers=n_layers, dropout=dropout, batch_first=True,bidirectional = True)\n",
        "        self.fc = nn.Linear(hidden_dim*2, hidden_dim)\n",
        "        self.fc = nn.Linear(hidden_dim,output_dim)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, text):\n",
        "        embedded = self.embedding(text)\n",
        "        output, (hidden, cell) = self.lstm(embedded)\n",
        "        hidden = self.dropout(hidden[-1])\n",
        "        output = self.fc(hidden)\n",
        "        return output\n",
        "\n",
        "# Step 6: Train the LSTM model\n",
        "def train(model, train_loader, valid_loader, optimizer, criterion, num_epochs):\n",
        "    best_model = None\n",
        "    best_f1 = 0.0\n",
        "    for epoch in range(num_epochs):\n",
        "        train_loss = 0.0\n",
        "        valid_loss = 0.0\n",
        "        model.train()\n",
        "        for inputs, labels in train_loader:\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs.squeeze(), labels.float())\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            train_loss += loss.item() * inputs.size(0)\n",
        "        train_loss /= len(train_loader.dataset)\n",
        "        model.eval()\n",
        "        with torch.no_grad():\n",
        "            for inputs, labels in valid_loader:\n",
        "                outputs = model(inputs)\n",
        "                loss = criterion(outputs.squeeze(), labels.float())\n",
        "                valid_loss += loss.item() * inputs.size(0)\n",
        "            valid_loss /= len(valid_loader.dataset)\n",
        "            valid_f1, valid_precision, valid_recall,valid_accuracy,cm = evaluate(model, valid_loader)\n",
        "            if valid_f1 > best_f1:\n",
        "                best_model = model\n",
        "                best_f1 = valid_f1\n",
        "        print(f'Epoch {epoch+1}: Train Loss: {train_loss:.4f}, Val Loss: {valid_loss:.4f}, Val F1: {valid_f1:.3f}, Val Precision: {valid_precision:.3f}, Val Recall: {valid_recall:.3f},Val Accuracy:{valid_accuracy:.3f}')\n",
        "    return best_model\n",
        "\n",
        "def evaluate(model, test_loader):\n",
        "    y_true = []\n",
        "    y_pred = []\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in test_loader:\n",
        "            outputs = model(inputs)\n",
        "            preds = torch.round(torch.sigmoid(outputs)).squeeze()\n",
        "            y_true.extend(labels.tolist())\n",
        "            y_pred.extend(preds.tolist())\n",
        "    f1 = f1_score(y_true, y_pred)\n",
        "    precision = precision_score(y_true, y_pred)\n",
        "    recall = recall_score(y_true, y_pred)\n",
        "    accuracy = accuracy_score(y_true,y_pred)\n",
        "    cm = confusion_matrix(y_true, y_pred)\n",
        "    return f1, precision, recall, accuracy,cm\n",
        "\n",
        "# Step 7: Grid search over hyperparameters\n",
        "vocab_size = len(tokenizer.word_index) + 1\n",
        "embedding_dim_values = [64]\n",
        "hidden_dim_values = [64,]\n",
        "dropout_values = [0.2]\n",
        "lr_values = np.arange(0.00001,0.001,0.001)\n",
        "num_epochs = 10\n",
        "n_layers =[1,2,3]\n",
        "best_model = None\n",
        "best_f1 = 0.0\n",
        "best_params = {}\n",
        "\n",
        "for embedding_dim, hidden_dim, dropout, lr, n_layers in product(embedding_dim_values, hidden_dim_values, dropout_values, lr_values, n_layers):\n",
        "    model = LSTMModel(vocab_size, embedding_dim, hidden_dim, output_dim=1, n_layers=n_layers, dropout=dropout)\n",
        "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
        "    criterion = nn.BCEWithLogitsLoss()\n",
        "    trained_model = train(model, train_loader, valid_loader, optimizer, criterion, num_epochs)\n",
        "    if trained_model is not None:\n",
        "        f1, precision, recall,accuracy,cm = evaluate(trained_model, test_loader)\n",
        "        #print(f'Embedding Dim: {embedding_dim}, Hidden Dim: {hidden_dim}, Dropout: {dropout}, LR: {lr}')\n",
        "        #print(f'Test F1: {f1:.3f}, Test Precision: {precision:.3f}, Test Recall: {recall:.3f}, Test Accuracy: {accuracy:.3f}')\n",
        "        if f1 > best_f1:\n",
        "            best_model = trained_model\n",
        "            best_f1 = f1\n",
        "            best_params = {\n",
        "                'embedding_dim': embedding_dim,\n",
        "                'hidden_dim': hidden_dim,\n",
        "                'lr':lr_values,\n",
        "                'dropout': dropout,\n",
        "                'learning_rate': lr,\n",
        "                'n_layers': n_layers}\n",
        "best_params\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "tuned_model = LSTMModel(vocab_size, embedding_dim=best_params['embedding_dim'], \n",
        "                        hidden_dim=best_params['hidden_dim'], output_dim=1, \n",
        "                        dropout=best_params['dropout'],n_layers=best_params['n_layers'])\n",
        "\n",
        "optimizer = optim.Adam(tuned_model.parameters(), lr=float(best_params['lr']))\n",
        "trained_model_tuned = train(tuned_model, train_loader, valid_loader, optimizer,criterion, num_epochs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "valid_f1, valid_precision, valid_recall,valid_accuracy,cm= evaluate(trained_model, test_loader)\n",
        "print(cm)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader\n",
        "from sklearn.metrics import f1_score, precision_score, recall_score\n",
        "\n",
        "# Step 5: Define the LSTM model\n",
        "class LSTMModel(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim, n_layers, dropout):\n",
        "        super().__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
        "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, num_layers=n_layers, dropout=dropout, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, text):\n",
        "        embedded = self.embedding(text)\n",
        "        output, (hidden, cell) = self.lstm(embedded)\n",
        "        hidden = self.dropout(hidden[-1])\n",
        "        output = self.fc(hidden)\n",
        "        return output\n",
        "\n",
        "# Step 6: Train the LSTM model\n",
        "def train(model, train_loader, valid_loader, optimizer, criterion, num_epochs):\n",
        "    for epoch in range(num_epochs):\n",
        "        train_loss = 0.0\n",
        "        valid_loss = 0.0\n",
        "        model.train()\n",
        "        for inputs, labels in train_loader:\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs.squeeze(), labels.float())\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            train_loss += loss.item() * inputs.size(0)\n",
        "        train_loss /= len(train_loader.dataset)\n",
        "        model.eval()\n",
        "        with torch.no_grad():\n",
        "            for inputs, labels in valid_loader:\n",
        "                outputs = model(inputs)\n",
        "                loss = criterion(outputs.squeeze(), labels.float())\n",
        "                valid_loss += loss.item() * inputs.size(0)\n",
        "            valid_loss /= len(valid_loader.dataset)\n",
        "            valid_f1, valid_precision, valid_recall = evaluate(model, valid_loader)\n",
        "        print(f'Epoch {epoch+1}: Train Loss: {train_loss:.4f}, Val Loss: {valid_loss:.4f}, Val F1: {valid_f1:.4f}, Val Precision: {valid_precision:.4f}, Val Recall: {valid_recall:.4f}')\n",
        "        \n",
        "        \n",
        "def evaluate(model, test_loader):\n",
        "    y_true = []\n",
        "    y_pred = []\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in test_loader:\n",
        "            outputs = model(inputs)\n",
        "            preds = torch.round(torch.sigmoid(outputs)).squeeze()\n",
        "            y_true.extend(labels.tolist())\n",
        "            y_pred.extend(preds.tolist())\n",
        "    f1 = f1_score(y_true, y_pred)\n",
        "    precision = precision_score(y_true, y_pred)\n",
        "    recall = recall_score(y_true, y_pred)\n",
        "    return f1, precision, recall\n",
        "\n",
        "# Step 7: Train the LSTM model\n",
        "vocab_size = len(tokenizer.word_index) + 1\n",
        "embedding_dim = 128\n",
        "hidden_dim = 64\n",
        "output_dim = 1\n",
        "n_layers = 2\n",
        "dropout = 0.5\n",
        "lr = 0.001\n",
        "num_epochs = 10\n",
        "\n",
        "model = LSTMModel(vocab_size, embedding_dim, hidden_dim, output_dim, n_layers, dropout)\n",
        "criterion = nn.BCEWithLogitsLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
        "\n",
        "train(model, train_loader, valid_loader, optimizer, criterion, num_epochs)\n",
        "\n",
        "# Step 8: Evaluate the LSTM model on the test set\n",
        "test_f1, test_precision, test_recall = evaluate(model, test_loader)\n",
        "print(f'Test F1: {test_f1:.4f}')\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iPufLVnBLsRu"
      },
      "source": [
        "### Part 1C:  Results for Methods\n",
        "\n",
        "F1, precision, etc."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jmoQeuQVLsRv"
      },
      "source": [
        "### Part 1D: Plots for Methods\n",
        "\n",
        "F1, precision, etc.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CQv5VRk3LsRv"
      },
      "source": [
        "## Part 2: Topic Modelling\n",
        "\n",
        "General comments and any shared processing here.\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Part 2A:Preprocessing"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### For Part 2 preprocessing, it will use the same method as part 1 i.e. preprocesser.preprocess under:\n",
        "\n",
        "##### Method 1 "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_train = pd.read_csv('train.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "prep = preprocesser()\n",
        "from nltk.stem.wordnet import WordNetLemmatizer\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "from nltk.tokenize import WhitespaceTokenizer\n",
        "from gensim.models import Phrases\n",
        "from gensim.corpora import Dictionary\n",
        "from gensim.models import LdaModel\n",
        "import pyLDAvis.gensim"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class topic_mod:\n",
        "    @staticmethod\n",
        "    def datasplit(Text, dataset, Size, SEED,Method):\n",
        "        df = dataset.sample(Size, random_state=SEED).reset_index(drop=True)\n",
        "        df_clean = pd.DataFrame(df[Text].apply(lambda x: prep.preprocess(x, Method=Method)))\n",
        "        docs = df_clean[Text].tolist()\n",
        "        return docs\n",
        "    def tokener(self,Text, dataset, Size, SEED,Method):\n",
        "        docs = self.datasplit(Text, dataset, Size, SEED,Method)\n",
        "        if Method == 1:\n",
        "            for idx in range(len(docs)):\n",
        "                docs[idx] = word_tokenize(docs[idx])  # Split into words\n",
        "        else:\n",
        "            tokenizer =WhitespaceTokenizer()\n",
        "            for idx in range(len(docs)):\n",
        "                docs[idx] = tokenizer.tokenize(docs[idx])  # Split into words.\n",
        "\n",
        "        # Remove words that are only one character.\n",
        "        docs = [[token for token in doc if len(token) > 1] for doc in docs]\n",
        "        docs = [[lemmatizer.lemmatize(token) for token in doc] for doc in docs]\n",
        "        return docs\n",
        "    \n",
        "    def LDA_train(self,Text, dataset, Size, SEED,Method,N_gram):\n",
        "        docs = self.tokener(Text, dataset, Size, SEED,Method)\n",
        "\n",
        "        if N_gram == 2:\n",
        "            bigram = Phrases(docs, min_count=10)\n",
        "            for idx in range(len(docs)):\n",
        "                for token in bigram[docs[idx]]:\n",
        "                    if '_' in token:\n",
        "                        # Token is a bigram, add to document.\n",
        "                        docs[idx].append(token)\n",
        "        else:\n",
        "            pass\n",
        "        dictionary = Dictionary(docs)\n",
        "        dictionary.filter_extremes(no_below=2, no_above=1)\n",
        "        corpus = [dictionary.doc2bow(doc) for doc in docs]\n",
        "        # Train LDA model.\n",
        "\n",
        "        # Set training parameters.\n",
        "        NUM_TOPICS = 30\n",
        "        chunksize = 2000\n",
        "        passes = 20\n",
        "        iterations = 400\n",
        "        eval_every = None  # Don't evaluate model perplexity, takes too much time.\n",
        "\n",
        "        # Make a index to word dictionary.\n",
        "        temp = dictionary[0]  # This is only to \"load\" the dictionary.\n",
        "        id2word = dictionary.id2token\n",
        "\n",
        "        model = LdaModel(\n",
        "            corpus=corpus,\n",
        "            id2word=id2word,\n",
        "            chunksize=chunksize,\n",
        "            alpha='auto',\n",
        "            eta='auto',\n",
        "            iterations=iterations,\n",
        "            num_topics=NUM_TOPICS,\n",
        "            passes=passes,\n",
        "            eval_every=eval_every\n",
        "        )\n",
        "        outputfile = f'model{NUM_TOPICS}.gensim'\n",
        "        print(\"Saving model in \" + outputfile)\n",
        "        print(\"\")\n",
        "        model.save(outputfile)\n",
        "\n",
        "        \n",
        "        lda_display = pyLDAvis.gensim.prepare(model, corpus, dictionary, sort_topics=False)\n",
        "        return pyLDAvis.display(lda_display)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "tm = topic_mod()\n",
        "docs = tm.LDA_train(Text = 'abstract', dataset = df_train, Size = 1000, SEED=SEED,Method = 1,N_gram = 2)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
